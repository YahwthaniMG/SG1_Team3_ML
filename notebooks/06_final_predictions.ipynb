{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction - Final Predictions\n",
    "# ==================================================\n",
    "\n",
    "# ## 📋 Objetivo\n",
    "# Generar predicciones finales usando nuestro modelo SVM entrenado:\n",
    "# - Cargar modelo final optimizado\n",
    "# - Aplicar pipeline completo de procesamiento\n",
    "# - Generar predicciones con análisis de confianza\n",
    "# - Crear archivo final de predicciones\n",
    "# - Validar resultados y generar insights finales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## 1. Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Funciones helper\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "from utils.helpers import save_current_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6g7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "## 2. Carga de Modelo Final y Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6g7h8i9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🤖 CARGANDO MODELO FINAL\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Cargar modelo entrenado\n",
    "model_path = \"../models/best_model_svm.pkl\"\n",
    "try:\n",
    "    best_model = joblib.load(model_path)\n",
    "    print(f\"✅ Modelo cargado exitosamente: {model_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: No se encuentra {model_path}\")\n",
    "    print(\"💡 Ejecuta primero el notebook 04_modeling.ipynb\")\n",
    "\n",
    "# Cargar métricas del modelo\n",
    "with open(\"../models/model_metrics.json\", \"r\") as f:\n",
    "    model_metrics = json.load(f)\n",
    "\n",
    "print(f\"\\n📊 INFORMACIÓN DEL MODELO:\")\n",
    "print(f\"  Algoritmo: {model_metrics['best_model']}\")\n",
    "print(f\"  Parámetros: {model_metrics['best_params']}\")\n",
    "print(f\"  Accuracy final: {model_metrics['final_test_metrics']['accuracy']:.4f}\")\n",
    "print(f\"  F1-Score: {model_metrics['final_test_metrics']['f1']:.4f}\")\n",
    "print(f\"  AUC-ROC: {model_metrics['final_test_metrics']['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g7h8i9j0",
   "metadata": {},
   "source": [
    "## 3. Función de Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "h8i9j0k1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_titanic_data(df_raw):\n",
    "    \"\"\"\n",
    "    Aplica exactamente el mismo pipeline de procesamiento usado en entrenamiento\n",
    "    \"\"\"\n",
    "    print(\"🔧 Aplicando pipeline de procesamiento...\")\n",
    "\n",
    "    # Copia para no modificar original\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # === PASO 1: DATA CLEANING ===\n",
    "    print(\"  📋 Paso 1: Data Cleaning\")\n",
    "\n",
    "    # Eliminar variables irrelevantes (si existen)\n",
    "    cols_to_drop = [\"PassengerId\", \"Name\", \"Ticket\"]\n",
    "    cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Manejar Cabin -> Cabin_Known\n",
    "    if \"Cabin\" in df.columns:\n",
    "        df[\"Cabin_Known\"] = df[\"Cabin\"].notna().astype(int)\n",
    "        df = df.drop(\"Cabin\", axis=1)\n",
    "\n",
    "    # Imputar Embarked con moda\n",
    "    if df[\"Embarked\"].isnull().any():\n",
    "        df[\"Embarked\"].fillna(\"S\", inplace=True)\n",
    "\n",
    "    # Imputar Age por grupo (Sex + Pclass)\n",
    "    if df[\"Age\"].isnull().any():\n",
    "        age_medians = df.groupby([\"Sex\", \"Pclass\"])[\"Age\"].median()\n",
    "\n",
    "        def impute_age(row):\n",
    "            if pd.isnull(row[\"Age\"]):\n",
    "                return age_medians[row[\"Sex\"], row[\"Pclass\"]]\n",
    "            return row[\"Age\"]\n",
    "\n",
    "        df[\"Age\"] = df.apply(impute_age, axis=1)\n",
    "\n",
    "    # === PASO 2: FEATURE ENGINEERING ===\n",
    "    print(\"  🔧 Paso 2: Feature Engineering\")\n",
    "\n",
    "    # Variables derivadas\n",
    "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "    df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n",
    "\n",
    "    # Age groups\n",
    "    def categorize_age(age):\n",
    "        if age < 12:\n",
    "            return \"Child\"\n",
    "        elif age < 18:\n",
    "            return \"Teen\"\n",
    "        elif age < 35:\n",
    "            return \"Young_Adult\"\n",
    "        elif age < 60:\n",
    "            return \"Middle_Age\"\n",
    "        else:\n",
    "            return \"Senior\"\n",
    "\n",
    "    df[\"AgeGroup\"] = df[\"Age\"].apply(categorize_age)\n",
    "\n",
    "    # Fare bins\n",
    "    df[\"FareBin\"] = pd.qcut(\n",
    "        df[\"Fare\"], q=4, labels=[\"Low\", \"Medium\", \"High\", \"Premium\"]\n",
    "    )\n",
    "\n",
    "    # Títulos - función mejorada para generar todos los títulos necesarios\n",
    "    def infer_title(row):\n",
    "        if row[\"Sex\"] == \"male\":\n",
    "            if row[\"Age\"] < 16:\n",
    "                return \"Master\"\n",
    "            elif row[\"Pclass\"] == 1 and row[\"Age\"] > 50:\n",
    "                # Aproximación para títulos de alta clase\n",
    "                return \"Officer\" if row[\"Fare\"] > 50 else \"Mr\"\n",
    "            else:\n",
    "                return \"Mr\"\n",
    "        else:  # female\n",
    "            if row[\"Age\"] < 16:\n",
    "                return \"Miss\"\n",
    "            elif row[\"SibSp\"] > 0:  # Married indicator\n",
    "                if row[\"Pclass\"] == 1 and row[\"Fare\"] > 100:\n",
    "                    return \"Royalty\"  # Aproximación para alta sociedad\n",
    "                else:\n",
    "                    return \"Mrs\"\n",
    "            else:\n",
    "                if row[\"Pclass\"] == 1 and row[\"Age\"] > 40:\n",
    "                    return \"Rare\"  # Aproximación para títulos raros\n",
    "                else:\n",
    "                    return \"Miss\"\n",
    "\n",
    "    df[\"Title_Simplified\"] = df.apply(infer_title, axis=1)\n",
    "\n",
    "    # Variables de interacción\n",
    "    df[\"Sex_Pclass\"] = df[\"Sex\"] + \"_Class\" + df[\"Pclass\"].astype(str)\n",
    "\n",
    "    def age_sex_category(row):\n",
    "        if row[\"AgeGroup\"] in [\"Child\", \"Teen\"]:\n",
    "            return \"Young\"\n",
    "        elif row[\"Sex\"] == \"female\":\n",
    "            return \"Adult_Female\"\n",
    "        else:\n",
    "            return \"Adult_Male\"\n",
    "\n",
    "    df[\"Age_Sex\"] = df.apply(age_sex_category, axis=1)\n",
    "\n",
    "    # === PASO 3: ENCODING ===\n",
    "    print(\"  🔢 Paso 3: Encoding\")\n",
    "\n",
    "    # Label encoding\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    le_sex = LabelEncoder()\n",
    "    le_embarked = LabelEncoder()\n",
    "\n",
    "    df[\"Sex_Encoded\"] = le_sex.fit_transform(df[\"Sex\"])\n",
    "    df[\"Embarked_Encoded\"] = le_embarked.fit_transform(df[\"Embarked\"])\n",
    "\n",
    "    # Ordinal encoding\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "    age_order = [\"Child\", \"Teen\", \"Young_Adult\", \"Middle_Age\", \"Senior\"]\n",
    "    oe_age = OrdinalEncoder(categories=[age_order])\n",
    "    df[\"AgeGroup_Encoded\"] = oe_age.fit_transform(df[[\"AgeGroup\"]]).astype(int)\n",
    "\n",
    "    fare_order = [\"Low\", \"Medium\", \"High\", \"Premium\"]\n",
    "    oe_fare = OrdinalEncoder(categories=[fare_order])\n",
    "    df[\"FareBin_Encoded\"] = oe_fare.fit_transform(df[[\"FareBin\"]]).astype(int)\n",
    "\n",
    "    # One-hot encoding\n",
    "    title_dummies = pd.get_dummies(df[\"Title_Simplified\"], prefix=\"Title\")\n",
    "    sex_pclass_dummies = pd.get_dummies(df[\"Sex_Pclass\"], prefix=\"SexPclass\")\n",
    "    age_sex_dummies = pd.get_dummies(df[\"Age_Sex\"], prefix=\"AgeSex\")\n",
    "\n",
    "    df = pd.concat([df, title_dummies, sex_pclass_dummies, age_sex_dummies], axis=1)\n",
    "\n",
    "    # === PASO 4: SELECCIÓN DE FEATURES FINALES ===\n",
    "    print(\"  🎯 Paso 4: Selección de Features\")\n",
    "\n",
    "    # Variables base\n",
    "    base_features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Cabin_Known\"]\n",
    "\n",
    "    # Features engineeradas\n",
    "    eng_features = [\"FamilySize\", \"IsAlone\"]\n",
    "\n",
    "    # Features encoded\n",
    "    encoded_features = [\n",
    "        \"Sex_Encoded\",\n",
    "        \"Embarked_Encoded\",\n",
    "        \"AgeGroup_Encoded\",\n",
    "        \"FareBin_Encoded\",\n",
    "    ]\n",
    "\n",
    "    # TODAS las columnas de títulos que el modelo espera\n",
    "    expected_title_columns = [\n",
    "        \"Title_Master\",\n",
    "        \"Title_Miss\",\n",
    "        \"Title_Mr\",\n",
    "        \"Title_Mrs\",\n",
    "        \"Title_Officer\",\n",
    "        \"Title_Rare\",\n",
    "        \"Title_Royalty\",\n",
    "    ]\n",
    "\n",
    "    # One-hot features esperadas\n",
    "    expected_sex_pclass = [\n",
    "        \"SexPclass_female_Class1\",\n",
    "        \"SexPclass_female_Class2\",\n",
    "        \"SexPclass_female_Class3\",\n",
    "        \"SexPclass_male_Class1\",\n",
    "        \"SexPclass_male_Class2\",\n",
    "        \"SexPclass_male_Class3\",\n",
    "    ]\n",
    "\n",
    "    expected_age_sex = [\"AgeSex_Adult_Female\", \"AgeSex_Adult_Male\", \"AgeSex_Young\"]\n",
    "\n",
    "    # Lista completa de columnas esperadas\n",
    "    expected_columns = (\n",
    "        base_features\n",
    "        + eng_features\n",
    "        + encoded_features\n",
    "        + expected_title_columns\n",
    "        + expected_sex_pclass\n",
    "        + expected_age_sex\n",
    "    )\n",
    "\n",
    "    # Crear DataFrame final con todas las columnas esperadas\n",
    "    df_final = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Añadir columnas existentes\n",
    "    for col in expected_columns:\n",
    "        if col in df.columns:\n",
    "            df_final[col] = df[col]\n",
    "        else:\n",
    "            # Añadir columnas faltantes con 0\n",
    "            df_final[col] = 0\n",
    "            print(f\"    ⚠️ Columna faltante añadida con 0: {col}\")\n",
    "\n",
    "    # === PASO 5: SCALING ===\n",
    "    print(\"  📏 Paso 5: Scaling\")\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    numeric_features = [\n",
    "        \"Pclass\",\n",
    "        \"Age\",\n",
    "        \"SibSp\",\n",
    "        \"Parch\",\n",
    "        \"Fare\",\n",
    "        \"FamilySize\",\n",
    "        \"Sex_Encoded\",\n",
    "        \"Embarked_Encoded\",\n",
    "        \"AgeGroup_Encoded\",\n",
    "        \"FareBin_Encoded\",\n",
    "    ]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_final[numeric_features] = scaler.fit_transform(df_final[numeric_features])\n",
    "\n",
    "    print(f\"  ✅ Procesamiento completado: {df_final.shape}\")\n",
    "    print(f\"  📋 Columnas finales: {len(df_final.columns)}\")\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "## 4. Carga de Datos para Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j0k1l2m3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n📊 CARGANDO DATOS PARA PREDICCIÓN\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Opción 1: Si tuvieras un test.csv separado (como en Kaggle)\n",
    "# test_data = pd.read_csv(\"../data/raw/test.csv\")\n",
    "\n",
    "# Opción 2: Usar una muestra del dataset original para demostración\n",
    "original_data = pd.read_csv(\"../data/raw/titanic.csv\")\n",
    "\n",
    "# Para demostración, usaremos los últimos 100 registros como \"test set\"\n",
    "demo_test_data = original_data.tail(100).copy()\n",
    "demo_test_passenger_ids = demo_test_data['PassengerId'].copy()\n",
    "\n",
    "# Simular que no conocemos las respuestas (eliminar Survived)\n",
    "demo_test_features = demo_test_data.drop('Survived', axis=1)\n",
    "true_labels = demo_test_data['Survived']  # Para validación\n",
    "\n",
    "print(f\"📋 Datos de prueba cargados:\")\n",
    "print(f\"  - Registros: {len(demo_test_features)}\")\n",
    "print(f\"  - Features: {len(demo_test_features.columns)}\")\n",
    "print(f\"  - PassengerIds: {demo_test_passenger_ids.min()} - {demo_test_passenger_ids.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4",
   "metadata": {},
   "source": [
    "## 5. Procesamiento de Datos de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l2m3n4o5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🔧 PROCESANDO DATOS DE TEST\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Aplicar pipeline completo\n",
    "test_processed = process_titanic_data(demo_test_features)\n",
    "\n",
    "print(f\"\\n✅ Datos procesados exitosamente:\")\n",
    "print(f\"  - Shape final: {test_processed.shape}\")\n",
    "print(f\"  - Features preparadas para el modelo\")\n",
    "\n",
    "# Verificar que no hay valores faltantes\n",
    "missing_values = test_processed.isnull().sum().sum()\n",
    "print(f\"  - Valores faltantes: {missing_values}\")\n",
    "\n",
    "if missing_values == 0:\n",
    "    print(\"  ✅ Sin valores faltantes - Listo para predicción\")\n",
    "else:\n",
    "    print(\"  ⚠️ Hay valores faltantes que deben manejarse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3n4o5p6",
   "metadata": {},
   "source": [
    "## 6. Generación de Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n4o5p6q7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🎯 GENERANDO PREDICCIONES\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Predicciones binarias\n",
    "predictions = best_model.predict(test_processed)\n",
    "print(f\"✅ Predicciones binarias generadas: {len(predictions)}\")\n",
    "\n",
    "# Probabilidades\n",
    "probabilities = best_model.predict_proba(test_processed)[:, 1]\n",
    "print(f\"✅ Probabilidades generadas: {len(probabilities)}\")\n",
    "\n",
    "# Resumen de predicciones\n",
    "n_survived = predictions.sum()\n",
    "n_died = len(predictions) - n_survived\n",
    "survival_rate = n_survived / len(predictions)\n",
    "\n",
    "print(f\"\\n📊 RESUMEN DE PREDICCIONES:\")\n",
    "print(f\"  - Total pasajeros: {len(predictions)}\")\n",
    "print(f\"  - Predichos como supervivientes: {n_survived} ({survival_rate:.1%})\")\n",
    "print(f\"  - Predichos como fallecidos: {n_died} ({1-survival_rate:.1%})\")\n",
    "print(f\"  - Probabilidad promedio: {probabilities.mean():.3f}\")\n",
    "print(\n",
    "    f\"  - Rango de probabilidades: [{probabilities.min():.3f}, {probabilities.max():.3f}]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o5p6q7r8",
   "metadata": {},
   "source": [
    "## 7. Análisis de Confianza de Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q7r8s9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🔍 ANÁLISIS DE CONFIANZA\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Categorizar por nivel de confianza\n",
    "def confidence_level(prob):\n",
    "    confidence = abs(prob - 0.5)  # Distancia del threshold\n",
    "    if confidence >= 0.4:\n",
    "        return 'Muy Alta'\n",
    "    elif confidence >= 0.3:\n",
    "        return 'Alta'\n",
    "    elif confidence >= 0.2:\n",
    "        return 'Media'\n",
    "    else:\n",
    "        return 'Baja'\n",
    "\n",
    "confidence_levels = [confidence_level(p) for p in probabilities]\n",
    "confidence_counts = pd.Series(confidence_levels).value_counts()\n",
    "\n",
    "print(\"Distribución de confianza:\")\n",
    "for level, count in confidence_counts.items():\n",
    "    print(f\"  - {level}: {count} predicciones ({count/len(predictions):.1%})\")\n",
    "\n",
    "# Casos de alta confianza\n",
    "high_confidence_mask = [confidence_level(p) == 'Muy Alta' for p in probabilities]\n",
    "high_confidence_count = sum(high_confidence_mask)\n",
    "\n",
    "print(f\"\\n🎯 Predicciones de muy alta confianza: {high_confidence_count}\")\n",
    "\n",
    "# Casos de baja confianza (más inciertos)\n",
    "low_confidence_mask = [confidence_level(p) == 'Baja' for p in probabilities]\n",
    "low_confidence_count = sum(low_confidence_mask)\n",
    "\n",
    "print(f\"⚠️ Predicciones de baja confianza: {low_confidence_count}\")\n",
    "\n",
    "# Visualización de distribución de probabilidades\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(probabilities, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', label='Threshold = 0.5')\n",
    "plt.xlabel('Probabilidad de Supervivencia')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Distribución de Probabilidades')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "confidence_counts.plot(kind='bar', color='lightgreen')\n",
    "plt.title('Distribución de Niveles de Confianza')\n",
    "plt.xlabel('Nivel de Confianza')\n",
    "plt.ylabel('Número de Predicciones')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "pd.Series(predictions).value_counts().plot(kind='bar', color=['crimson', 'forestgreen'])\n",
    "plt.title('Predicciones Finales')\n",
    "plt.xlabel('Predicción (0=Muerte, 1=Supervivencia)')\n",
    "plt.ylabel('Número de Pasajeros')\n",
    "plt.xticks([0, 1], ['Fallecidos', 'Supervivientes'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_current_plot('final_predictions_analysis', '../results/figures/final_predictions/')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abd244",
   "metadata": {},
   "source": [
    "## 8. Validación con Etiquetas Verdaderas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n✅ VALIDACIÓN DE PREDICCIONES\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Como tenemos las etiquetas verdaderas, podemos validar\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Métricas de validación\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"📊 PERFORMANCE EN DATOS DE DEMO:\")\n",
    "print(f\"  - Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "\n",
    "# Reporte detallado\n",
    "print(f\"\\n📋 Reporte de clasificación:\")\n",
    "print(\n",
    "    classification_report(\n",
    "        true_labels, predictions, target_names=[\"Fallecidos\", \"Supervivientes\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "print(f\"\\n🔍 Matriz de confusión:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualización de matriz de confusión\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Fallecidos\", \"Supervivientes\"],\n",
    "    yticklabels=[\"Fallecidos\", \"Supervivientes\"],\n",
    ")\n",
    "plt.title(\"Matriz de Confusión - Datos Demo\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Valor Real\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Comparación de distribuciones\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Real\": true_labels.value_counts().sort_index(),\n",
    "        \"Predicho\": pd.Series(predictions).value_counts().sort_index(),\n",
    "    }\n",
    ")\n",
    "comparison_df.plot(kind=\"bar\", ax=plt.gca())\n",
    "plt.title(\"Comparación: Real vs Predicho\")\n",
    "plt.xlabel(\"Supervivencia (0=No, 1=Sí)\")\n",
    "plt.ylabel(\"Número de Pasajeros\")\n",
    "plt.xticks([0, 1], [\"Fallecidos\", \"Supervivientes\"], rotation=0)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "save_current_plot(\"validation_results\", \"../results/figures/final_predictions/\")\n",
    "plt.show()\n",
    "\n",
    "# Análisis de casos bien/mal clasificados\n",
    "correct_predictions = (true_labels == predictions).sum()\n",
    "incorrect_predictions = len(predictions) - correct_predictions\n",
    "\n",
    "print(f\"\\n🎯 ANÁLISIS DE ACIERTOS:\")\n",
    "print(\n",
    "    f\"  - Predicciones correctas: {correct_predictions} ({correct_predictions/len(predictions):.1%})\"\n",
    ")\n",
    "print(\n",
    "    f\"  - Predicciones incorrectas: {incorrect_predictions} ({incorrect_predictions/len(predictions):.1%})\"\n",
    ")\n",
    "\n",
    "# Comparar con performance esperada del modelo\n",
    "expected_accuracy = model_metrics[\"final_test_metrics\"][\"accuracy\"]\n",
    "performance_diff = accuracy - expected_accuracy\n",
    "\n",
    "print(f\"\\n📈 COMPARACIÓN CON PERFORMANCE ESPERADA:\")\n",
    "print(f\"  - Accuracy esperada: {expected_accuracy:.4f}\")\n",
    "print(f\"  - Accuracy obtenida: {accuracy:.4f}\")\n",
    "print(f\"  - Diferencia: {performance_diff:+.4f}\")\n",
    "\n",
    "if abs(performance_diff) < 0.05:\n",
    "    print(\"  ✅ Performance consistente con el modelo entrenado\")\n",
    "else:\n",
    "    print(\"  ⚠️ Variación significativa - normal en muestras pequeñas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9424ce",
   "metadata": {},
   "source": [
    "## 9. Casos Interesantes de Análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce60efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🔍 ANÁLISIS DE CASOS INTERESANTES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"PassengerId\": demo_test_passenger_ids,\n",
    "        \"Prediction\": predictions,\n",
    "        \"Probability\": probabilities,\n",
    "        \"Confidence\": confidence_levels,\n",
    "        \"True_Label\": true_labels,\n",
    "        \"Correct\": (true_labels == predictions),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Añadir información demográfica original\n",
    "demo_info = demo_test_data[\n",
    "    [\"PassengerId\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "].copy()\n",
    "results_df = results_df.merge(demo_info, on=\"PassengerId\")\n",
    "\n",
    "print(\"🎯 TOP 5 PREDICCIONES MÁS CONFIADAS (SUPERVIVIENTES):\")\n",
    "top_survivors = results_df[results_df[\"Prediction\"] == 1].nlargest(5, \"Probability\")\n",
    "for idx, row in top_survivors.iterrows():\n",
    "    status = \"✅\" if row[\"Correct\"] else \"❌\"\n",
    "    print(\n",
    "        f\"  {status} ID {row['PassengerId']}: {row['Sex']}, Clase {row['Pclass']}, Edad {row['Age']:.0f} - Prob: {row['Probability']:.3f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n💀 TOP 5 PREDICCIONES MÁS CONFIADAS (FALLECIDOS):\")\n",
    "top_deaths = results_df[results_df[\"Prediction\"] == 0].nsmallest(5, \"Probability\")\n",
    "for idx, row in top_deaths.iterrows():\n",
    "    status = \"✅\" if row[\"Correct\"] else \"❌\"\n",
    "    print(\n",
    "        f\"  {status} ID {row['PassengerId']}: {row['Sex']}, Clase {row['Pclass']}, Edad {row['Age']:.0f} - Prob: {row['Probability']:.3f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n⚠️ TOP 5 PREDICCIONES MENOS CONFIADAS (MÁS INCIERTAS):\")\n",
    "uncertain_cases = results_df.iloc[(results_df[\"Probability\"] - 0.5).abs().argsort()[:5]]\n",
    "for idx, row in uncertain_cases.iterrows():\n",
    "    status = \"✅\" if row[\"Correct\"] else \"❌\"\n",
    "    pred_text = \"Supervive\" if row[\"Prediction\"] == 1 else \"Muere\"\n",
    "    print(\n",
    "        f\"  {status} ID {row['PassengerId']}: {row['Sex']}, Clase {row['Pclass']}, Edad {row['Age']:.0f} - Pred: {pred_text} (Prob: {row['Probability']:.3f})\"\n",
    "    )\n",
    "\n",
    "# Análisis por características demográficas\n",
    "print(f\"\\n📊 ANÁLISIS POR CARACTERÍSTICAS:\")\n",
    "\n",
    "# Por género\n",
    "gender_analysis = (\n",
    "    results_df.groupby(\"Sex\")\n",
    "    .agg({\"Prediction\": \"mean\", \"Probability\": \"mean\", \"Correct\": \"mean\"})\n",
    "    .round(3)\n",
    ")\n",
    "print(\"\\nPor género:\")\n",
    "print(gender_analysis)\n",
    "\n",
    "# Por clase\n",
    "class_analysis = (\n",
    "    results_df.groupby(\"Pclass\")\n",
    "    .agg({\"Prediction\": \"mean\", \"Probability\": \"mean\", \"Correct\": \"mean\"})\n",
    "    .round(3)\n",
    ")\n",
    "print(\"\\nPor clase:\")\n",
    "print(class_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e1e624",
   "metadata": {},
   "source": [
    "## 10. Creación de Archivo de Predicciones Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5709eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n💾 CREANDO ARCHIVO DE PREDICCIONES FINALES\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Crear directorio si no existe\n",
    "import os\n",
    "\n",
    "os.makedirs(\"../results/final_predictions\", exist_ok=True)\n",
    "\n",
    "# Archivo de submission estilo Kaggle\n",
    "submission = pd.DataFrame(\n",
    "    {\"PassengerId\": demo_test_passenger_ids, \"Survived\": predictions}\n",
    ")\n",
    "\n",
    "submission_path = \"../results/final_predictions/titanic_predictions.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"✅ Archivo de predicciones guardado: {submission_path}\")\n",
    "\n",
    "# Archivo detallado con probabilidades y análisis\n",
    "detailed_results = pd.DataFrame(\n",
    "    {\n",
    "        \"PassengerId\": demo_test_passenger_ids,\n",
    "        \"Predicted_Survival\": predictions,\n",
    "        \"Survival_Probability\": probabilities.round(4),\n",
    "        \"Confidence_Level\": confidence_levels,\n",
    "        \"Passenger_Class\": demo_test_data[\"Pclass\"].values,\n",
    "        \"Sex\": demo_test_data[\"Sex\"].values,\n",
    "        \"Age\": demo_test_data[\"Age\"].values,\n",
    "        \"Fare\": demo_test_data[\"Fare\"].values,\n",
    "        \"Family_Size\": demo_test_data[\"SibSp\"].values\n",
    "        + demo_test_data[\"Parch\"].values\n",
    "        + 1,\n",
    "        \"Embarked\": demo_test_data[\"Embarked\"].values,\n",
    "    }\n",
    ")\n",
    "\n",
    "detailed_path = \"../results/final_predictions/detailed_predictions.csv\"\n",
    "detailed_results.to_csv(detailed_path, index=False)\n",
    "print(f\"✅ Archivo detallado guardado: {detailed_path}\")\n",
    "\n",
    "# Archivo de métricas y resumen\n",
    "summary_stats = {\n",
    "    \"model_used\": model_metrics[\"best_model\"],\n",
    "    \"model_params\": model_metrics[\"best_params\"],\n",
    "    \"model_training_accuracy\": model_metrics[\"final_test_metrics\"][\"accuracy\"],\n",
    "    \"predictions_generated\": len(predictions),\n",
    "    \"predicted_survivors\": int(predictions.sum()),\n",
    "    \"predicted_deaths\": int(len(predictions) - predictions.sum()),\n",
    "    \"survival_rate_predicted\": float(predictions.mean()),\n",
    "    \"average_probability\": float(probabilities.mean()),\n",
    "    \"high_confidence_predictions\": int(\n",
    "        sum([c == \"Muy Alta\" for c in confidence_levels])\n",
    "    ),\n",
    "    \"low_confidence_predictions\": int(sum([c == \"Baja\" for c in confidence_levels])),\n",
    "    \"validation_accuracy\": float(accuracy),\n",
    "    \"generation_timestamp\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "summary_path = \"../results/final_predictions/prediction_summary.json\"\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "print(f\"✅ Resumen guardado: {summary_path}\")\n",
    "\n",
    "print(f\"\\n📋 ARCHIVOS GENERADOS:\")\n",
    "print(f\"  1. {submission_path} - Formato Kaggle submission\")\n",
    "print(f\"  2. {detailed_path} - Análisis detallado con probabilidades\")\n",
    "print(f\"  3. {summary_path} - Resumen y métricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025bf5ec",
   "metadata": {},
   "source": [
    "## 11. Insights Finales y Storytelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f37831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🎭 INSIGHTS FINALES Y STORYTELLING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"🚢 HISTORIA DEL MODELO TITANIC:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "model_story = f\"\"\"\n",
    "Nuestro modelo SVM, entrenado con {model_metrics['best_model']} y optimizado con los parámetros\n",
    "{model_metrics['best_params']}, ha demostrado una capacidad excepcional para predecir \n",
    "la supervivencia en el Titanic con un {model_metrics['final_test_metrics']['accuracy']*100:.1f}% de precisión.\n",
    "\n",
    "En esta demostración con {len(predictions)} pasajeros:\n",
    "• Predijimos que {predictions.sum()} sobrevivirían ({predictions.mean()*100:.1f}%)\n",
    "• {sum([c == 'Muy Alta' for c in confidence_levels])} predicciones fueron de muy alta confianza\n",
    "• El modelo mantuvo {accuracy*100:.1f}% de precisión en estos datos de prueba\n",
    "\n",
    "Los patrones históricos capturados por nuestro modelo reflejan la realidad social de 1912:\n",
    "• Las mujeres de primera clase tuvieron las mayores probabilidades de supervivencia\n",
    "• Los hombres de tercera clase enfrentaron las menores probabilidades\n",
    "• El protocolo \"mujeres y niños primero\" se refleja claramente en las predicciones\n",
    "\"\"\"\n",
    "\n",
    "print(model_story)\n",
    "\n",
    "print(\"\\n🎯 CASOS DESTACADOS:\")\n",
    "\n",
    "# Caso más confiado de supervivencia\n",
    "most_confident_survivor = results_df[results_df[\"Prediction\"] == 1].loc[\n",
    "    results_df[\"Probability\"].idxmax()\n",
    "]\n",
    "print(f\"\\n👑 SUPERVIVIENTE MÁS PROBABLE:\")\n",
    "print(\n",
    "    f\"   Pasajero ID {most_confident_survivor['PassengerId']}: {most_confident_survivor['Sex']}, {most_confident_survivor['Age']:.0f} años, Clase {most_confident_survivor['Pclass']}\"\n",
    ")\n",
    "print(f\"   Probabilidad: {most_confident_survivor['Probability']:.1%}\")\n",
    "print(f\"   Perfil: Representa el arquetipo de mayor supervivencia según nuestro modelo\")\n",
    "\n",
    "# Caso más confiado de muerte\n",
    "most_confident_death = results_df[results_df[\"Prediction\"] == 0].loc[\n",
    "    results_df[\"Probability\"].idxmin()\n",
    "]\n",
    "print(f\"\\n💀 FALLECIMIENTO MÁS PROBABLE:\")\n",
    "print(\n",
    "    f\"   Pasajero ID {most_confident_death['PassengerId']}: {most_confident_death['Sex']}, {most_confident_death['Age']:.0f} años, Clase {most_confident_death['Pclass']}\"\n",
    ")\n",
    "print(f\"   Probabilidad de supervivencia: {most_confident_death['Probability']:.1%}\")\n",
    "print(f\"   Perfil: Representa las circunstancias más desfavorables de la época\")\n",
    "\n",
    "# Caso más incierto\n",
    "most_uncertain = results_df.iloc[\n",
    "    (results_df[\"Probability\"] - 0.5).abs().argsort().iloc[0]\n",
    "]\n",
    "print(f\"\\n❓ CASO MÁS INCIERTO:\")\n",
    "print(\n",
    "    f\"   Pasajero ID {most_uncertain['PassengerId']}: {most_uncertain['Sex']}, {most_uncertain['Age']:.0f} años, Clase {most_uncertain['Pclass']}\"\n",
    ")\n",
    "print(\n",
    "    f\"   Probabilidad: {most_uncertain['Probability']:.1%} (muy cerca del umbral 50%)\"\n",
    ")\n",
    "print(\n",
    "    f\"   Insight: Casos como este muestran la complejidad humana detrás de las estadísticas\"\n",
    ")\n",
    "\n",
    "print(f\"\\n🔮 VALOR PREDICTIVO DEL MODELO:\")\n",
    "print(f\"Este modelo no solo predice supervivencia, sino que revela:\")\n",
    "print(f\"• Patrones sociales de 1912 capturados cuantitativamente\")\n",
    "print(f\"• La intersección de género, clase social y edad como factores determinantes\")\n",
    "print(f\"• Casos excepcionales que desafían las normas estadísticas\")\n",
    "print(f\"• Lecciones aplicables a protocolos de emergencia modernos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c9465",
   "metadata": {},
   "source": [
    "## 12. Resumen y Próximos Pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e496b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🎯 RESUMEN DE PREDICCIONES FINALES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "final_summary = f\"\"\"\n",
    "✅ MISIÓN CUMPLIDA - PREDICCIONES FINALES GENERADAS\n",
    "\n",
    "📊 ESTADÍSTICAS FINALES:\n",
    "    • Modelo utilizado: {model_metrics['best_model']}\n",
    "    • Predicciones generadas: {len(predictions)}\n",
    "    • Accuracy de validación: {accuracy:.1%}\n",
    "    • Predicciones de alta confianza: {sum([c == 'Muy Alta' for c in confidence_levels])}\n",
    "    • Archivos generados: 3 (submission, detallado, resumen)\n",
    "\n",
    "🎓 VALOR ACADÉMICO DEMOSTRADO:\n",
    "    • Pipeline completo de ML implementado exitosamente\n",
    "    • Modelo productivo capaz de generar predicciones nuevas\n",
    "    • Análisis de confianza y validación incluidos\n",
    "    • Storytelling histórico conectado con resultados técnicos\n",
    "\n",
    "🚀 APLICABILIDAD PRÁCTICA:\n",
    "    • Modelo listo para ser utilizado en nuevos datos del Titanic\n",
    "    • Pipeline reproducible para otros datasets similares\n",
    "    • Metodología aplicable a análisis históricos con ML\n",
    "    • Insights valiosos para diseño de protocolos de emergencia\n",
    "\"\"\"\n",
    "\n",
    "print(final_summary)\n",
    "\n",
    "print(\"\\n🔄 POSIBLES EXTENSIONES:\")\n",
    "extensions = [\n",
    "    \"🌐 Aplicar modelo a otros datasets históricos de naufragios\",\n",
    "    \"📊 Crear dashboard interactivo con Streamlit/Dash\",\n",
    "    \"🤖 Implementar ensemble con múltiples modelos\",\n",
    "    \"📱 Desarrollar API REST para predicciones en tiempo real\",\n",
    "    \"📚 Ampliar análisis con datos adicionales de Encyclopedia Titanica\",\n",
    "    \"🎯 Optimizar modelo para diferentes métricas (recall, precision)\",\n",
    "]\n",
    "\n",
    "for ext in extensions:\n",
    "    print(f\"   {ext}\")\n",
    "\n",
    "print(f\"\\n🏆 CONCLUSIÓN:\")\n",
    "print(f\"El modelo Titanic ha demostrado no solo capacidad predictiva técnica,\")\n",
    "print(f\"sino también la habilidad de revelar patrones históricos significativos.\")\n",
    "print(f\"Cada predicción cuenta una historia humana respaldada por datos.\")\n",
    "\n",
    "print(f\"\\n✨ ¡Proyecto de Machine Learning completado exitosamente! ✨\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
